Register hook for shared (<class 'torch.nn.modules.sparse.Embedding'>) at depth 0
Register hook for embed_tokens (<class 'torch.nn.modules.sparse.Embedding'>) at depth 1
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 1 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 1 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 1 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 1 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 1 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 1 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for final_layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 1
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 1
Register hook for embed_tokens (<class 'torch.nn.modules.sparse.Embedding'>) at depth 1
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for EncDecAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 2 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for EncDecAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 2 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for EncDecAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 2 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for EncDecAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 2 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for EncDecAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 2 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for SelfAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for EncDecAttention (<class 'transformers.models.t5.modeling_t5.T5Attention'>) at depth 5
Register hook for layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 5
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 5
Register hook for 2 (<class 'transformers.models.t5.modeling_t5.T5LayerFF'>) at depth 4
Register hook for final_layer_norm (<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>) at depth 1
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 1
pre_memory: 252.087861328125 MB
<class 'torch.nn.modules.sparse.Embedding'>:0:shared                        	 Time: 0.086212 ms 	 Memory: 514.69 MB
<class 'torch.nn.modules.sparse.Embedding'>:1:embed_tokens                  	 Time: 0.730147 ms 	 Memory: 1029.38 MB
<class 'torch.nn.modules.dropout.Dropout'>:1:dropout                        	 Time: 0.032218 ms 	 Memory: 1050.84 MB
<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>:5:layer_norm       	 Time: 1.656716 ms 	 Memory: 4743.89 MB
<class 'transformers.models.t5.modeling_t5.T5Attention'>:5:SelfAttention    	 Time: 13.638196 ms 	 Memory: 3149.35 MB
<class 'torch.nn.modules.dropout.Dropout'>:5:dropout                        	 Time: 0.138917 ms 	 Memory: 4753.08 MB
<class 'transformers.models.t5.modeling_t5.T5LayerFF'>:4:1                  	 Time: 1.434672 ms 	 Memory: 1554.20 MB
<class 'transformers.models.t5.modeling_t5.T5LayerNorm'>:1:final_layer_norm 	 Time: 0.182447 ms 	 Memory: 536.14 MB
<class 'transformers.models.t5.modeling_t5.T5Attention'>:5:EncDecAttention  	 Time: 2.535808 ms 	 Memory: 1603.72 MB
<class 'transformers.models.t5.modeling_t5.T5LayerFF'>:4:2                  	 Time: 1.416588 ms 	 Memory: 1607.48 MB
Total time: 3.124976 s
