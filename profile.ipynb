{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Union\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "from utils import plot_layer_profiling, plot_layer_profiling_dist, COLOR_MAP, get_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_latency_res = pd.read_csv('profile_res/latency_bert-base-uncased_res.csv')\n",
    "gpt2_latency_res = pd.read_csv('profile_res/latency_gpt2_res.csv')\n",
    "# bart_latency_res = pd.read_csv('profile_res/latency_bart-base_res.csv')\n",
    "\n",
    "# bert_memory_res = pd.read_csv('profile_res/memory_bert-base-uncased_res.csv')\n",
    "gpt2_memory_res = pd.read_csv('profile_res/memory_gpt2_res.csv')\n",
    "# bart_memory_res = pd.read_csv('profile_res/memory_bart-base_res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_profiling_split(\n",
    "    profile_res: pd.DataFrame, \n",
    "    model_name: str, \n",
    "    save_file: str = None,\n",
    "    color_map: dict = None,\n",
    "    metric: str = 'inference latency',\n",
    "    unit: str = 'seconds',\n",
    "):\n",
    "    if color_map is None:\n",
    "        color_map = {\n",
    "            'embedding_layer': 'blue',\n",
    "            'ln_1': 'orange',\n",
    "            'attn': 'purple',\n",
    "            'ln_2': 'green',\n",
    "            'mlp': 'brown',\n",
    "            'FC_layer': 'red',\n",
    "        }\n",
    "    \n",
    "    # Create custom patches for legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color_map[key], label=key) for key in color_map]\n",
    "    \n",
    "    # Split data based on input_length\n",
    "    # Quantiles for splitting\n",
    "    # q1 = profile_res['input_length'].quantile(0.1)\n",
    "    # q2 = profile_res['input_length'].quantile(0.5)\n",
    "    # small_data = profile_res[profile_res['input_length'] <= q1].drop(columns=['batch_size', 'input_length'])\n",
    "    # normal_data = profile_res[(profile_res['input_length'] > q1) & (profile_res['input_length'] <= q2)].drop(columns=['batch_size', 'input_length'])\n",
    "    # large_data = profile_res[profile_res['input_length'] > q2].drop(columns=['batch_size', 'input_length'])\n",
    "    small_data = profile_res[profile_res['input_length'] <= 128].drop(columns=['batch_size', 'input_length'], ignore_errors=True)\n",
    "    normal_data = profile_res[(profile_res['input_length'] > 128) & (profile_res['input_length'] <= 512)].drop(columns=['batch_size', 'input_length'], ignore_errors=True)\n",
    "    large_data = profile_res[profile_res['input_length'] > 512].drop(columns=['batch_size', 'input_length'], ignore_errors=True)\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    data_splits = [('Small', small_data), ('Normal', normal_data), ('Large', large_data)]\n",
    "    for i, (split_name, data) in enumerate(data_splits, 1):\n",
    "        plt.subplot(3, 1, i)\n",
    "        \n",
    "        # Determine the color of each column based on its label\n",
    "        column_colors = [color_map['embedding_layer'] if 'embedding' in idx \n",
    "                        else color_map['ln_1'] if 'ln_1' in idx\n",
    "                        else color_map['attn'] if 'attn' in idx\n",
    "                        else color_map['ln_2'] if 'ln_2' in idx\n",
    "                        else color_map['mlp'] if 'mlp' in idx\n",
    "                        else color_map['FC_layer'] if 'FC' in idx \n",
    "                        else 'grey'  # default color \n",
    "                        for idx in data.columns]\n",
    "\n",
    "        # Box plot of the data without outliers\n",
    "        boxprops = dict(linestyle='-', linewidth=1)\n",
    "        medianprops = dict(linestyle='-', linewidth=2, color='black')\n",
    "        bp = data.boxplot(vert=True, patch_artist=True, boxprops=boxprops, medianprops=medianprops, showfliers=False, return_type='dict')\n",
    "        \n",
    "        # Coloring the boxes based on the determined colors\n",
    "        for patch, color in zip(bp['boxes'], column_colors):\n",
    "            patch.set_facecolor(color)\n",
    "\n",
    "        plt.ylabel(f'{metric} ({unit})', fontdict={'fontsize': 12})\n",
    "        plt.xlabel('Layer', fontdict={'fontsize': 12})\n",
    "        plt.title(f'Distribution of {metric} for {split_name} Input Length in {model_name}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(axis='y')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Custom legend only for the first subplot\n",
    "        if i == 1:\n",
    "            plt.legend(handles=legend_elements, title=\"Layer type\")\n",
    "    \n",
    "    if save_file:\n",
    "        plt.savefig(save_file, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_chart(\n",
    "    profile_res: pd.DataFrame, \n",
    "    model_name: str, \n",
    "    save_file: str = None,\n",
    "    color_map: dict = None,\n",
    "    metric: str = 'inference latency',\n",
    "):\n",
    "    if color_map is None:\n",
    "        color_map = {\n",
    "            'embedding_layer': 'blue',\n",
    "            'ln_1': 'orange',\n",
    "            'attn': 'purple',\n",
    "            'ln_2': 'green',\n",
    "            'mlp': 'brown',\n",
    "            'FC_layer': 'red',\n",
    "        }\n",
    "    \n",
    "    # Extract only the layer columns (excluding batch_size and input_length)\n",
    "    res = profile_res.drop(columns=['batch_size', 'input_length'], ignore_errors=True)\n",
    "    \n",
    "    # Dictionary to store the summed latency for each layer type\n",
    "    layer_sum = {\n",
    "        'embedding_layer': 0,\n",
    "        'ln_1': 0,\n",
    "        'attn': 0,\n",
    "        'ln_2': 0,\n",
    "        'mlp': 0,\n",
    "        'FC_layer': 0\n",
    "    }\n",
    "    \n",
    "    # Sum up latencies for each layer type\n",
    "    for column in res.columns:\n",
    "        if 'embedding' in column:\n",
    "            layer_sum['embedding_layer'] += res[column].sum()\n",
    "        elif 'ln_1' in column:\n",
    "            layer_sum['ln_1'] += res[column].sum()\n",
    "        elif 'attn' in column:\n",
    "            layer_sum['attn'] += res[column].sum()\n",
    "        elif 'ln_2' in column:\n",
    "            layer_sum['ln_2'] += res[column].sum()\n",
    "        elif 'mlp' in column:\n",
    "            layer_sum['mlp'] += res[column].sum()\n",
    "        elif 'FC' in column:\n",
    "            layer_sum['FC_layer'] += res[column].sum()\n",
    "    \n",
    "    # Pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # plt.pie(layer_sum.values(), labels=layer_sum.keys(), autopct='%1.1f%%', startangle=140, colors=[color_map[key] for key in layer_sum.keys()])\n",
    "    patches, texts, autotexts = plt.pie(layer_sum.values(), autopct='%1.1f%%', startangle=140, colors=[color_map[key] for key in layer_sum.keys()])\n",
    "    plt.title(f'Layer-wise Latency Distribution for {model_name}')\n",
    "    plt.title(f'Percentage of {metric} for different types of layers in {model_name}')\n",
    "    \n",
    "    # Add legend\n",
    "    # legend_labels = [f\"{key.replace('_', ' ').title()}\" for key in layer_sum.keys()]\n",
    "    legend_labels = [f\"{key}\" for key in layer_sum.keys()]\n",
    "    plt.legend(patches, legend_labels, loc=\"best\", title=\"Layer Type\")\n",
    "    \n",
    "    # Improve appearance\n",
    "    for t in texts:\n",
    "        t.set(size=10)\n",
    "    for at in autotexts:\n",
    "        at.set(size=10, weight=\"bold\")\n",
    "    \n",
    "    if save_file:\n",
    "        plt.savefig(save_file, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_charts(\n",
    "    profile_res: pd.DataFrame, \n",
    "    model_name: str, \n",
    "    save_file: str = None,\n",
    "    color_map: dict = None,\n",
    "    metric: str = 'inference latency',\n",
    "    unit: str = 'seconds',\n",
    "):\n",
    "    if color_map is None:\n",
    "        color_map = {\n",
    "            'embedding_layer': 'blue',\n",
    "            'ln_1': 'orange',\n",
    "            'attn': 'purple',\n",
    "            'ln_2': 'green',\n",
    "            'mlp': 'brown',\n",
    "            'FC_layer': 'red',\n",
    "        }\n",
    "    \n",
    "    # Split data into Small, Normal, and Large subsets\n",
    "    small_data = profile_res[profile_res['input_length'] <= 128].drop(\n",
    "        columns=['batch_size', 'input_length'], \n",
    "        ignore_errors=True,\n",
    "    )\n",
    "    normal_data = profile_res[(profile_res['input_length'] > 128) & (profile_res['input_length'] <= 512)].drop(\n",
    "        columns=['batch_size', 'input_length'],\n",
    "        ignore_errors=True,\n",
    "    )\n",
    "    large_data = profile_res[profile_res['input_length'] > 512].drop(\n",
    "        columns=['batch_size', 'input_length'], \n",
    "        ignore_errors=True,\n",
    "    )\n",
    "    \n",
    "    data_subsets = [small_data, normal_data, large_data]\n",
    "    subset_names = ['Small', 'Normal', 'Large']\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(20, 18), width_ratios=[3, 1])\n",
    "    \n",
    "    for i, (data, subset_name) in enumerate(zip(data_subsets, subset_names)):\n",
    "        # First column: box plots\n",
    "        column_colors = [color_map['embedding_layer'] if 'embedding' in idx \n",
    "                else color_map['ln_1'] if 'ln_1' in idx\n",
    "                else color_map['attn'] if 'attn' in idx\n",
    "                else color_map['ln_2'] if 'ln_2' in idx\n",
    "                else color_map['mlp'] if 'mlp' in idx\n",
    "                else color_map['FC_layer'] if 'FC' in idx \n",
    "                else 'grey' for idx in data.columns]\n",
    "        \n",
    "        # Boxplot\n",
    "        boxprops = dict(linestyle='-', linewidth=1)\n",
    "        medianprops = dict(linestyle='-', linewidth=2, color='black')\n",
    "        bp = data.boxplot(ax=axes[i, 0], vert=True, patch_artist=True, boxprops=boxprops, medianprops=medianprops, showfliers=False, return_type='dict')\n",
    "        # Coloring the boxes based on the determined colors\n",
    "        for patch, color in zip(bp['boxes'], column_colors):\n",
    "            patch.set_facecolor(color)\n",
    "            \n",
    "        axes[i, 0].set_title(f'{subset_name} Input Length: Distribution of Latency for {model_name}')\n",
    "        axes[i, 0].set_xlabel('Layer', fontdict={'fontsize': 12})\n",
    "        axes[i, 0].set_ylabel(f'{metric} ({unit})', fontdict={'fontsize': 12})\n",
    "        axes[i, 0].set_xticklabels(data.columns, rotation=45, ha='right')\n",
    "        axes[i, 0].grid(axis='y')\n",
    "        \n",
    "        # Second column: pie charts\n",
    "        # Sum up latencies for each layer type\n",
    "        layer_sum = {\n",
    "            'embedding_layer': 0,\n",
    "            'ln_1': 0,\n",
    "            'attn': 0,\n",
    "            'ln_2': 0,\n",
    "            'mlp': 0,\n",
    "            'FC_layer': 0\n",
    "        }\n",
    "        for column in data.columns:\n",
    "            if 'embedding' in column:\n",
    "                layer_sum['embedding_layer'] += data[column].sum()\n",
    "            elif 'ln_1' in column:\n",
    "                layer_sum['ln_1'] += data[column].sum()\n",
    "            elif 'attn' in column:\n",
    "                layer_sum['attn'] += data[column].sum()\n",
    "            elif 'ln_2' in column:\n",
    "                layer_sum['ln_2'] += data[column].sum()\n",
    "            elif 'mlp' in column:\n",
    "                layer_sum['mlp'] += data[column].sum()\n",
    "            elif 'FC' in column:\n",
    "                layer_sum['FC_layer'] += data[column].sum()\n",
    "        \n",
    "        # Pie chart\n",
    "        patches, texts, autotexts = axes[i, 1].pie(layer_sum.values(), autopct='%1.1f%%', startangle=140, colors=[color_map[key] for key in layer_sum.keys()])\n",
    "        \n",
    "        # Improve appearance of pie chart\n",
    "        for t in texts:\n",
    "            t.set(size=10)\n",
    "        for at in autotexts:\n",
    "            at.set(size=10, weight=\"bold\")\n",
    "    \n",
    "    # Add legend for pie chart\n",
    "    legend_labels = [f\"{key}\" for key in layer_sum.keys()]\n",
    "    fig.legend(patches, legend_labels, loc='upper right', title=\"Layer Type\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_file:\n",
    "        plt.savefig(save_file, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('figures'):\n",
    "    os.makedirs('figures')\n",
    "\n",
    "plot_layer_profiling(\n",
    "    bert_latency_res, \n",
    "    'BERT-base', \n",
    "    save_file='figures/bert_layer_latency.pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    bert_memory_res, \n",
    "    'BERT-base', \n",
    "    save_file='figures/bert_layer_memory.pdf',\n",
    "    metric='inference memory',\n",
    "    unit='GB',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    gpt2_latency_res, \n",
    "    'GPT2-small', \n",
    "    save_file='figures/gpt2_layer_latency.pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    gpt2_memory_res, \n",
    "    'GPT2-small', \n",
    "    save_file='figures/gpt2_layer_memory.pdf',\n",
    "    metric='inference memory',\n",
    "    unit='GB',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling_dist(\n",
    "    gpt2_latency_res, \n",
    "    'GPT2-small', \n",
    "    save_file='figures/gpt2_layer_latency_dist.pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling_dist(\n",
    "    gpt2_memory_res, \n",
    "    'GPT2-small', \n",
    "    save_file='figures/gpt2_layer_memory_dist.pdf',\n",
    "    metric='inference memory',\n",
    "    unit='GB',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling_split(\n",
    "    gpt2_latency_res, \n",
    "    'GPT2-small', \n",
    "    save_file='figures/gpt2_layer_latency_dist_split.pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling_split(\n",
    "    gpt2_memory_res, \n",
    "    'GPT2-small', \n",
    "    save_file='figures/gpt2_layer_memory_dist_split.pdf',\n",
    "    metric='inference memory',\n",
    "    unit='GB',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_charts(\n",
    "    gpt2_latency_res,\n",
    "    'GPT2-small',\n",
    "    save_file='figures/gpt2_layer_latency_split_combined.pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_charts(\n",
    "    gpt2_memory_res,\n",
    "    'GPT2-small',\n",
    "    save_file='figures/gpt2_layer_memory_split_combined.pdf',\n",
    "    metric='inference memory',\n",
    "    unit='GB',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie_chart(\n",
    "    gpt2_latency_res,\n",
    "    'GPT2-small',\n",
    "    save_file='figures/gpt2_pie_latency.pdf',\n",
    "    metric='inference latency',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie_chart(\n",
    "    gpt2_memory_res,\n",
    "    'GPT2-small',\n",
    "    save_file='figures/gpt2_pie_memory.pdf',\n",
    "    metric='inference memory',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    bart_latency_res, \n",
    "    'BART-base', \n",
    "    save_file='figures/bart_layer_latency.pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    bart_memory_res, \n",
    "    'BART-base', \n",
    "    save_file='figures/bart_layer_memory.pdf',\n",
    "    metric='inference memory',\n",
    "    unit='GB',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_bert = pd.DataFrame(json.load(open('profile_res/latency_forward_bert-base-uncased.json')))\n",
    "memory_bert = pd.DataFrame(json.load(open('profile_res/memory_forward_bert-base-uncased.json')))\n",
    "input_len_bert = pd.DataFrame(json.load(open('profile_res/input_length_bert-base-uncased.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    latency_bert, \n",
    "    'Bert-base-uncased', \n",
    "    save_file='figures/bert_layer_latency_forward_average.pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling_dist(\n",
    "    latency_bert, \n",
    "    'Bert-base-uncased', \n",
    "    save_file='figures/bert_layer_latency_forward_dist.pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'results/text-classification/bert-base-uncased'\n",
    "forward_input_len_bert = pd.DataFrame(json.load(open(f'{output_dir}/train_input_length.json')))\n",
    "forward_latency_bert = pd.DataFrame(json.load(open(f'{output_dir}/train_forward_latency.json')))\n",
    "forward_memory_bert = pd.DataFrame(json.load(open(f'{output_dir}/train_forward_memory.json')))\n",
    "backward_latency_bert = pd.DataFrame(json.load(open(f'{output_dir}/train_backward_latency.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_MAP = {\n",
    "    'embedding': 'blue',\n",
    "    'attention': 'purple',\n",
    "    'ffn': 'brown',\n",
    "    'dropout': 'grey',\n",
    "    'backward': 'green',\n",
    "}\n",
    "\n",
    "def get_colors(index: List[str], color_map: dict = COLOR_MAP):\n",
    "    return [\n",
    "        color_map['embedding'] if 'embedding' in idx.lower() \n",
    "        else color_map['attention'] if 'attention' in idx.lower() or 'attn' in idx.lower()\n",
    "        else color_map['layernorm'] if 'ln' in idx.lower() or 'layernorm' in idx.lower()\n",
    "        else color_map['ffn'] if (\n",
    "            'mlp' in idx.lower() or \n",
    "            'linear' in idx.lower() or \n",
    "            'pooler' in idx.lower() or \n",
    "            'intermediate' in idx.lower() or\n",
    "            'output' in idx.lower()\n",
    "        )\n",
    "        else color_map['dropout'] if 'dropout' in idx.lower()\n",
    "        else color_map['backward'] if 'backward' in idx.lower() or 'bp' in idx.lower()\n",
    "        else 'red'  # default color \n",
    "        for idx in index]\n",
    "\n",
    "# Plot the average latency distribution of each layer\n",
    "def plot_layer_profiling(\n",
    "    profile_res: pd.DataFrame, \n",
    "    model_name: str, \n",
    "    backward_res: pd.DataFrame = None,\n",
    "    save_file: str = None,\n",
    "    color_map: dict = COLOR_MAP,\n",
    "    metric: str = 'inference latency',\n",
    "    unit: str = 'seconds',\n",
    "    figsize: Tuple[int, int] = (20, 6),\n",
    "):\n",
    "    # Assuming you have the DataFrame loaded as df (do not include the batch_size, input_length columns)\n",
    "    if 'batch_size' in profile_res.columns and 'input_length' in profile_res.columns:\n",
    "        res = profile_res.drop(columns=['batch_size', 'input_length'])\n",
    "    else:\n",
    "        res = profile_res\n",
    "    averages = res.mean()\n",
    "    if backward_res is not None:\n",
    "        averages['backward'] = backward_res.mean().tolist()[0]\n",
    "    \n",
    "    # Determine the color of each bar based on its label\n",
    "    colors = get_colors(averages.index)\n",
    "    \n",
    "    # Create custom patches for legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color_map[key], label=key) for key in color_map]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=figsize)\n",
    "    averages.plot(kind='bar', color=colors, width=0.5)\n",
    "    \n",
    "    # Also plot line graph\n",
    "    plt.plot(averages, color='black', linestyle='-', linewidth=2)\n",
    "    \n",
    "    plt.ylabel(f'Average {metric} ({unit})', fontdict={'fontsize': 12})\n",
    "    plt.xlabel('Layer', fontdict={'fontsize': 12})\n",
    "    plt.title(f'Average {metric} per Layer for {model_name}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    # Add legend for the 6 layers\n",
    "    plt.legend(handles=legend_elements, title=\"Layer type\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_file:\n",
    "        plt.savefig(save_file, bbox_inches='tight')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    forward_latency_bert, \n",
    "    'Bert-base-uncased', \n",
    "    backward_res=backward_latency_bert,\n",
    "    save_file='figures/bert_layer_latency_forward_average.pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    forward_memory_bert/(1024 ** 2), \n",
    "    'Bert-base-uncased', \n",
    "    save_file='figures/bert_layer_memory_forward_average.pdf',\n",
    "    metric='inference memory',\n",
    "    unit='MB',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'results/text-classification/bert-base-uncased'\n",
    "forward_input_len_bert = pd.DataFrame(json.load(open(f'{output_dir}/eval_input_length.json')))\n",
    "forward_latency_bert = pd.DataFrame(json.load(open(f'{output_dir}/eval_forward_latency.json')))\n",
    "forward_memory_bert = pd.DataFrame(json.load(open(f'{output_dir}/eval_forward_memory.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    forward_latency_bert, \n",
    "    'Bert-base-uncased', \n",
    "    save_file='figures/bert_layer_latency_forward_average(eval).pdf',\n",
    "    metric='inference latency',\n",
    "    unit='seconds',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_profiling(\n",
    "    forward_memory_bert/(1024 ** 2), \n",
    "    'Bert-base-uncased', \n",
    "    save_file='figures/bert_layer_memory_forward_average(eval).pdf',\n",
    "    metric='inference memory',\n",
    "    unit='MB',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'results/text-classification/bert-base-uncased'\n",
    "df = []\n",
    "selection_sizes = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "for batch_size in selection_sizes:\n",
    "    df.append(json.load(open(f'{dir}/train_backward_latency(batch={batch_size}).json')))\n",
    "    \n",
    "df = pd.DataFrame(np.array(df).T, columns=selection_sizes)\n",
    "df\n",
    "\n",
    "# Plot the average latency distribution regarding each batch size\n",
    "plt.figure(figsize=(5, 3))\n",
    "df.mean().plot(kind='bar', color='blue', width=0.3)\n",
    "plt.ylabel('Average Inference Latency (seconds)', fontdict={'fontsize': 8})\n",
    "plt.xlabel('Batch Size', fontdict={'fontsize': 8})\n",
    "plt.title('Avg. backward latency per selection size for BERT-base-uncased', fontdict={'fontsize': 8})\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/bert_backward_latency_selection_size (B).pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "dir = 'results-profile/text-classification/bert-base-uncased'\n",
    "batch = 16\n",
    "data_mean = {}\n",
    "data_sum = {}\n",
    "\n",
    "for part in [\n",
    "    'forward',\n",
    "    'backward',\n",
    "    'optimization',\n",
    "    # 'others',\n",
    "]:\n",
    "    if part != 'others':\n",
    "        data_mean[part] = {}\n",
    "    data_sum[part] = {}\n",
    "    for method in [\n",
    "        'basic',\n",
    "        'grad_accum',\n",
    "        'back_accum',\n",
    "    ]:\n",
    "        if part == 'others':\n",
    "            train_df = json.load(open(f'{dir}/{method}/train_results.json'))\n",
    "            data_sum['others'][method] = train_df['train_runtime'] - data_sum['forward'][method] - data_sum['backward'][method] - data_sum['optimization'][method]\n",
    "        else:\n",
    "            accum = 1 if method == 'basic' else batch\n",
    "            df = json.load(open(f'{dir}/{method}/{part}(batch={batch},accum={accum}).json'))\n",
    "            mean = np.mean(df)\n",
    "            sum = np.sum(df)\n",
    "            print(f'{method}-{part} times: {len(df)}')\n",
    "            data_mean[part][method] = mean * 1000\n",
    "            data_sum[part][method] = sum\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(6, 4))\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "width = 0.2\n",
    "\n",
    "# Plotting avg latencies\n",
    "parts_mean = list(data_mean.keys())\n",
    "methods = list(data_mean[parts_mean[0]].keys())\n",
    "means = [[data_mean[part][method] for method in methods] for part in parts_mean]\n",
    "ind = np.arange(len(parts_mean))\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    bars = ax[0].bar(ind + i*width, [means[j][i] for j in range(len(parts_mean))], width, label=method)\n",
    "    # Add numbers on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax[0].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                '%.1f' % float(height),\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "\n",
    "ax[0].set_xticks(ind + width)\n",
    "ax[0].set_xticklabels(parts_mean)\n",
    "ax[0].set_ylabel('Avg. latency (milliseconds)')\n",
    "ax[0].legend(loc='best', title='Method')\n",
    "\n",
    "# Plotting sum latencies\n",
    "parts_sum = list(data_sum.keys())\n",
    "sums = [[data_sum[part][method] for method in methods] for part in parts_sum]\n",
    "ind = np.arange(len(parts_sum))\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    bars = ax[1].bar(ind + i*width, [sums[j][i] for j in range(len(parts_sum))], width, label=method)\n",
    "    # Add numbers on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax[1].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                '%.1f' % float(height),\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "ax[1].set_xticks(ind + width)\n",
    "ax[1].set_xticklabels(parts_sum)\n",
    "ax[1].set_ylabel('Total latency (seconds)')\n",
    "ax[1].legend(loc='best', title='Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/train_latency_breakdown.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# Safely getting values from the data dictionary\n",
    "sums = {method: [data_sum[part][method] for part in parts_sum] for method in methods}\n",
    "\n",
    "# Plot pie charts for each method\n",
    "fig, axs = plt.subplots(1, len(methods), figsize=(15, 5))\n",
    "\n",
    "for i, (method, values) in enumerate(sums.items()):\n",
    "    axs[i].pie(values, labels=parts_sum, autopct='%1.1f%%', startangle=90, wedgeprops=dict(width=0.3))\n",
    "    axs[i].set_title(method)\n",
    "\n",
    "plt.suptitle('Proportion of Latencies by part for Each Method', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/train_latency_breakdown_proportion.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(basic_latencies, color='blue', linestyle='-', linewidth=2, label='Standard')\n",
    "plt.plot(grad_accum_latencies, color='red', linestyle='-', linewidth=2, label='Gradient Accumulation')\n",
    "plt.plot(back_accum_latencies, color='green', linestyle='-', linewidth=2, label='Backward Accumulation')\n",
    "plt.ylabel('Backward latency (seconds)', fontdict={'fontsize': 8})\n",
    "plt.xlabel('Times', fontdict={'fontsize': 8})\n",
    "plt.title(f'Backward latency distribution for BERT-base-uncased (batch={batch}, accum={accum})', fontdict={'fontsize': 8})\n",
    "plt.xticks(rotation=0, fontsize=8)\n",
    "plt.grid(axis='y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figures/backward_latency_distribution(batch={batch},accum={accum}).pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average latency distribution regarding each batch size\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "basic_average = np.mean(basic_latencies)\n",
    "grad_accum_average = np.mean(grad_accum_latencies)\n",
    "back_accum_average = np.mean(back_accum_latencies)\n",
    "\n",
    "plt.bar('Standard', basic_average, color='blue', width=0.2)\n",
    "plt.bar('Gradient Accumulation', grad_accum_average, color='red', width=0.2)\n",
    "plt.bar('Backward Accumulation', back_accum_average, color='green', width=0.2)\n",
    "# Add numbers on top of each bar\n",
    "plt.text(x=-0.07, y=basic_average+0.01, s=f'{basic_average:.2f}', size=8)\n",
    "plt.text(x=0.92, y=grad_accum_average+0.01, s=f'{grad_accum_average:.2f}', size=8)\n",
    "plt.text(x=1.92, y=back_accum_average+0.01, s=f'{back_accum_average:.2f}', size=8)\n",
    "\n",
    "plt.ylabel('Average backward latency (seconds)', fontdict={'fontsize': 8})\n",
    "plt.xlabel('Method', fontdict={'fontsize': 8})\n",
    "plt.title(f'Avg. backward latency per method for BERT-base-uncased (batch={batch}, accum={accum})', fontdict={'fontsize': 8})\n",
    "plt.xticks(rotation=0, fontsize=8)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figures/backward_latency_avg.pdf', bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(basic_latencies), len(grad_accum_latencies), len(back_accum_latencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"Hello, my dog is cute\",\n",
    "    \"Hello, I like your hat, where did you get it?\",\n",
    "    \"What day is it today?\",\n",
    "    \"How are you doing?\",\n",
    "    \"I am doing great!\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"Can you please help me with the homework? I am stuck.\",\n",
    "    \"I am going to the park.\",\n",
    "    \"I am going to the park with my friends.\",\n",
    "    \"You are the most beautiful person I have ever met.\",\n",
    "]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "methods = [\n",
    "    'all',\n",
    "    'random',\n",
    "    'entropy',\n",
    "    'vanilla',\n",
    "]\n",
    "dataset = 'mrpc'\n",
    "results = []\n",
    "cur_epoch = None\n",
    "for method in methods:\n",
    "    trainer_state = json.load(open(f'results/{dataset}/bert-base-cased/{method}/trainer_state.json'))\n",
    "    for entry in trainer_state['log_history']:\n",
    "        # print(entry)\n",
    "        if 'loss' in entry:\n",
    "            record = {}\n",
    "            record['method'] = method\n",
    "            record['epoch'] = entry['epoch']\n",
    "            record['step'] = entry['step']\n",
    "            record['train_loss'] = entry['loss']\n",
    "        elif 'eval_accuracy' in entry:\n",
    "            record['eval_accuracy'] = entry['eval_accuracy'] * 100\n",
    "            record['eval_f1'] = entry['eval_f1'] * 100\n",
    "            results.append(record)\n",
    "        \n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metric vs. epoch for different methods\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Create a list of metrics to plot\n",
    "metrics = ['train_loss', 'eval_accuracy', 'eval_f1']\n",
    "# Plotting\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(x='step', y=metric, hue='method', data=results)\n",
    "    # plt.title(f'{metric.replace(\"_\", \" \").capitalize()} vs. training step')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").capitalize())\n",
    "    plt.legend(title='Method')\n",
    "    plt.savefig(f'figures/{metric}_vs_step.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchs = [2, 4, 6, 8, 10, 12, 14]\n",
    "method = 'entropy'\n",
    "dataset = 'mrpc'\n",
    "results = []\n",
    "cur_epoch = None\n",
    "for batch in batchs:\n",
    "    trainer_state = json.load(open(f'results/{dataset}/bert-base-cased/{method}/{batch}/trainer_state.json'))\n",
    "    for entry in trainer_state['log_history']:\n",
    "        # print(entry)\n",
    "        if 'loss' in entry:\n",
    "            record = {}\n",
    "            record['batch_size'] = batch\n",
    "            record['epoch'] = entry['epoch']\n",
    "            record['step'] = entry['step']\n",
    "            record['train_loss'] = entry['loss']\n",
    "        elif 'eval_accuracy' in entry:\n",
    "            record['eval_accuracy'] = entry['eval_accuracy'] * 100\n",
    "            record['eval_f1'] = entry['eval_f1'] * 100\n",
    "            results.append(record)\n",
    "        \n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Create a list of metrics to plot\n",
    "metrics = ['train_loss', 'eval_accuracy', 'eval_f1']\n",
    "# Plotting\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(x='step', y=metric, hue='batch_size', data=results)\n",
    "    # plt.title(f'{metric.replace(\"_\", \" \").capitalize()} vs. training step')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").capitalize())\n",
    "    plt.legend(title='Batch size')\n",
    "    plt.savefig(f'figures/{metric}_vs_step_entropy.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset to return IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset = load_dataset('glue', 'mrpc')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dataset['train']['idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss1 = CrossEntropyLoss()\n",
    "loss2 = CrossEntropyLoss(reduction='none')\n",
    "logits = torch.rand(5, 5000)\n",
    "l = torch.randint(0, 5000, (5,))\n",
    "print(loss1(logits.view(-1, 5000), l.view(-1)))\n",
    "print(loss2(logits.view(-1, 5000), l.view(-1)))\n",
    "print(loss2(logits.view(-1, 5000), l.view(-1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(10, 5000)\n",
    "model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "ouputs = model(torch.rand(5, 10))\n",
    "print(ouputs.shape)\n",
    "loss = loss2(ouputs, l.view(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for single_loss in loss:\n",
    "    print(single_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='results/bert-base-cased',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    greater_is_better=True,\n",
    ")\n",
    "args.num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test distributed llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "# # In this way, the parameters are always the same\n",
    "# access_token = \"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\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# config = AutoConfig.from_pretrained(model_name_or_path, token=access_token)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, token=access_token)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name_or_path, \n",
    "#     config=config, \n",
    "#     token=access_token,\n",
    "#     device_map='auto',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys \n",
    "# sys.dont_write_bytecode = True\n",
    "# from collections import defaultdict\n",
    "# from models.utils import get_stages\n",
    "# num_stages = 8\n",
    "# timing_info = defaultdict(list)\n",
    "# stages = get_stages(config, num_stages, timing_info=timing_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
