Register hook for wte (<class 'torch.nn.modules.sparse.Embedding'>) at depth 0
Register hook for wpe (<class 'torch.nn.modules.sparse.Embedding'>) at depth 0
Register hook for drop (<class 'torch.nn.modules.dropout.Dropout'>) at depth 0
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_1 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for attn (<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>) at depth 2
Register hook for ln_2 (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 2
Register hook for mlp (<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>) at depth 2
Register hook for ln_f (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 0
pre_memory: 526.7366650390625 MB
<class 'torch.nn.modules.sparse.Embedding'>:0:wte                     	 Time: 0.031860 ms 	 Memory: 526.84 MB
<class 'torch.nn.modules.sparse.Embedding'>:0:wpe                     	 Time: 0.021954 ms 	 Memory: 526.87 MB
<class 'torch.nn.modules.dropout.Dropout'>:0:drop                     	 Time: 0.008922 ms 	 Memory: 526.97 MB
<class 'torch.nn.modules.normalization.LayerNorm'>:2:ln_1             	 Time: 0.345211 ms 	 Memory: 6541.96 MB
<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>:2:attn 	 Time: 12.601230 ms 	 Memory: 6552.28 MB
<class 'torch.nn.modules.normalization.LayerNorm'>:2:ln_2             	 Time: 0.344985 ms 	 Memory: 6554.82 MB
<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>:2:mlp        	 Time: 2.657905 ms 	 Memory: 6581.40 MB
<class 'torch.nn.modules.normalization.LayerNorm'>:0:ln_f             	 Time: 0.028143 ms 	 Memory: 566.53 MB
Total time: 2.298289 s
