Register hook for word_embeddings (<class 'torch.nn.modules.sparse.Embedding'>) at depth 1
Register hook for position_embeddings (<class 'torch.nn.modules.sparse.Embedding'>) at depth 1
Register hook for token_type_embeddings (<class 'torch.nn.modules.sparse.Embedding'>) at depth 1
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 1
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 1
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for attention (<class 'transformers.models.bert.modeling_bert.BertAttention'>) at depth 3
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for intermediate_act_fn (<class 'transformers.activations.GELUActivation'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 4
Register hook for LayerNorm (<class 'torch.nn.modules.normalization.LayerNorm'>) at depth 4
Register hook for dropout (<class 'torch.nn.modules.dropout.Dropout'>) at depth 4
Register hook for dense (<class 'torch.nn.modules.linear.Linear'>) at depth 1
Register hook for activation (<class 'torch.nn.modules.activation.Tanh'>) at depth 1
pre_memory: 443.791298828125 MB
<class 'torch.nn.modules.activation.Tanh'>:1:activation                    	 Time: 0.021155 ms 	 Memory: 469.12 MB
<class 'torch.nn.modules.dropout.Dropout'>:1:dropout                       	 Time: 0.008423 ms 	 Memory: 444.34 MB
<class 'torch.nn.modules.dropout.Dropout'>:4:dropout                       	 Time: 0.091698 ms 	 Memory: 5489.83 MB
<class 'torch.nn.modules.linear.Linear'>:1:dense                           	 Time: 0.048125 ms 	 Memory: 469.11 MB
<class 'torch.nn.modules.linear.Linear'>:4:dense                           	 Time: 1.172738 ms 	 Memory: 10971.92 MB
<class 'torch.nn.modules.normalization.LayerNorm'>:1:LayerNorm             	 Time: 0.030067 ms 	 Memory: 444.34 MB
<class 'torch.nn.modules.normalization.LayerNorm'>:4:LayerNorm             	 Time: 0.334725 ms 	 Memory: 5492.93 MB
<class 'torch.nn.modules.sparse.Embedding'>:1:position_embeddings          	 Time: 0.020759 ms 	 Memory: 444.21 MB
<class 'torch.nn.modules.sparse.Embedding'>:1:token_type_embeddings        	 Time: 0.021451 ms 	 Memory: 444.05 MB
<class 'torch.nn.modules.sparse.Embedding'>:1:word_embeddings              	 Time: 0.032620 ms 	 Memory: 443.92 MB
<class 'transformers.activations.GELUActivation'>:4:intermediate_act_fn    	 Time: 0.232520 ms 	 Memory: 5488.28 MB
<class 'transformers.models.bert.modeling_bert.BertAttention'>:3:attention 	 Time: 12.752829 ms 	 Memory: 5475.91 MB
Total time: 2.461915 s
